{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End: Serve scikit-learn model with OpenVINO on Red Hat OpenShift AI\n",
    "\n",
    "This notebook performs the full workflow:\n",
    "1. Upload an existing **`retail_sales_model.joblib`** to **MinIO**\n",
    "2. Convert it to **ONNX** (with `skl2onnx`)\n",
    "3. Upload the ONNX using an **OVMS-compatible versioned layout**\n",
    "4. Deploy an **OpenVINO Model Server (OVMS)** InferenceService on **OpenShift AI (KServe)**\n",
    "5. Wait for readiness and **run a test inference** via the **V2 Inference Protocol**\n",
    "\n",
    "âœ… *This notebook does **not** set a namespace; it uses the current OpenShift project context.*\n",
    "\n",
    "----\n",
    "### Prerequisites\n",
    "- Access to a running **MinIO** endpoint reachable from this environment\n",
    "- Your trained scikit-learn pipeline saved as `retail_sales_model.joblib`\n",
    "- OpenShift cluster with **Red Hat OpenShift AI** and **KServe** installed\n",
    "- Permissions to create Secrets/ServiceAccounts/InferenceServices in your current project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if missing\n",
    "try:\n",
    "    import minio  # noqa: F401\n",
    "except Exception:\n",
    "    %pip install --quiet minio\n",
    "try:\n",
    "    import joblib  # noqa: F401\n",
    "except Exception:\n",
    "    %pip install --quiet joblib\n",
    "try:\n",
    "    import skl2onnx  # noqa: F401\n",
    "except Exception:\n",
    "    %pip install --quiet skl2onnx onnx\n",
    "try:\n",
    "    import sklearn  # noqa: F401\n",
    "except Exception:\n",
    "    %pip install --quiet scikit-learn\n",
    "try:\n",
    "    import kubernetes  # noqa: F401\n",
    "except Exception:\n",
    "    %pip install --quiet kubernetes\n",
    "try:\n",
    "    import yaml  # noqa: F401\n",
    "except Exception:\n",
    "    %pip install --quiet pyyaml\n",
    "try:\n",
    "    import requests  # noqa: F401\n",
    "except Exception:\n",
    "    %pip install --quiet requests\n",
    "\n",
    "print('Dependencies ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure connection and paths\n",
    "Set these environment variables or edit below. `MINIO_SECURE=false` is fine for a quick start; use `true` + TLS in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    model_local_path: str\n",
    "    model_name: str\n",
    "    minio_endpoint: str\n",
    "    minio_access_key: str\n",
    "    minio_secret_key: str\n",
    "    minio_bucket: str\n",
    "    minio_secure: bool\n",
    "    s3_prefix: str  # e.g., 'retail'\n",
    "\n",
    "cfg = Config(\n",
    "    model_local_path=os.getenv('MODEL_LOCAL_PATH', '/mnt/data/retail_sales_model.joblib'),\n",
    "    model_name=os.getenv('MODEL_NAME', 'retail-sales'),\n",
    "    minio_endpoint=os.getenv('MINIO_ENDPOINT'),\n",
    "    minio_access_key=os.getenv('MINIO_ACCESS_KEY'),\n",
    "    minio_secret_key=os.getenv('MINIO_SECRET_KEY'),\n",
    "    minio_bucket=os.getenv('MINIO_BUCKET', 'models'),\n",
    "    minio_secure=os.getenv('MINIO_SECURE', 'false').lower() == 'true',\n",
    "    s3_prefix=os.getenv('S3_PREFIX', 'retail')\n",
    ")\n",
    "cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the original `.joblib` to MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from minio import Minio\n",
    "\n",
    "model_path = Path(cfg.model_local_path)\n",
    "assert model_path.exists(), f\"Model file not found at {model_path}. Place retail_sales_model.joblib there or set MODEL_LOCAL_PATH.\"\n",
    "\n",
    "s3 = Minio(cfg.minio_endpoint, access_key=cfg.minio_access_key, secret_key=cfg.minio_secret_key, secure=cfg.minio_secure)\n",
    "if not s3.bucket_exists(cfg.minio_bucket):\n",
    "    s3.make_bucket(cfg.minio_bucket)\n",
    "    print(f\"Created bucket '{cfg.minio_bucket}'\")\n",
    "else:\n",
    "    print(f\"Bucket '{cfg.minio_bucket}' exists\")\n",
    "\n",
    "joblib_key = f\"{cfg.s3_prefix}/{model_path.name}\"\n",
    "s3.fput_object(cfg.minio_bucket, joblib_key, str(model_path), content_type='application/octet-stream')\n",
    "print('Uploaded joblib to s3://%s/%s' % (cfg.minio_bucket, joblib_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert scikit-learn pipeline to ONNX\n",
    "This assumes your `joblib` contains either the pipeline directly or a dict with a `model` key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib, numpy as np\n",
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType, StringTensorType, Int64TensorType\n",
    "\n",
    "loaded = joblib.load(model_path)\n",
    "pipeline = loaded.get('model') if isinstance(loaded, dict) and 'model' in loaded else loaded\n",
    "print(type(pipeline))\n",
    "\n",
    "# Infer input feature width if available\n",
    "n_features = getattr(pipeline, 'n_features_in_', None)\n",
    "if n_features is None:\n",
    "    # Fallback: let the user set FEATURE_COUNT env or default to 10\n",
    "    n_features = int(os.getenv('FEATURE_COUNT', '10'))\n",
    "print('Using feature count:', n_features)\n",
    "\n",
    "initial_types = [(\"input\", FloatTensorType([None, n_features]))]\n",
    "onnx_model = convert_sklearn(pipeline, initial_types=initial_types)\n",
    "\n",
    "onnx_dir = Path('ovms_model')/ '1'\n",
    "onnx_dir.mkdir(parents=True, exist_ok=True)\n",
    "onnx_path = onnx_dir / 'model.onnx'\n",
    "with open(onnx_path, 'wb') as f:\n",
    "    f.write(onnx_model.SerializeToString())\n",
    "print('Saved ONNX at', onnx_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload ONNX to MinIO (OVMS layout)\n",
    "OVMS expects a versioned directory structure: `<model_root>/<version>/model.onnx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openvino_root_key = f\"{cfg.s3_prefix}/openvino\"  # e.g., retail/openvino\n",
    "target_key = f\"{openvino_root_key}/1/model.onnx\"\n",
    "s3.fput_object(cfg.minio_bucket, target_key, str(onnx_path), content_type='application/octet-stream')\n",
    "print('Uploaded ONNX to s3://%s/%s' % (cfg.minio_bucket, target_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Secret, ServiceAccount, and OVMS InferenceService (KServe)\n",
    "This uses the Kubernetes Python client. It assumes your Workbench is running in-cluster with a valid service account (or `oc login`).\n",
    "\n",
    "**Note:** No namespace is set explicitly; resources are created in the current project (namespace) configured for this environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64, time, yaml\n",
    "from kubernetes import client, config\n",
    "\n",
    "def b64(s: str):\n",
    "    return base64.b64encode(s.encode()).decode()\n",
    "\n",
    "# Attempt to load in-cluster or local kubeconfig\n",
    "try:\n",
    "    config.load_incluster_config()\n",
    "    print('Loaded in-cluster kube config')\n",
    "except Exception:\n",
    "    config.load_kube_config()\n",
    "    print('Loaded local kube config')\n",
    "\n",
    "core = client.CoreV1Api()\n",
    "custom = client.CustomObjectsApi()\n",
    "\n",
    "# Determine current namespace (try serviceaccount namespace file; else default)\n",
    "ns = 'default'\n",
    "try:\n",
    "    with open('/var/run/secrets/kubernetes.io/serviceaccount/namespace') as f:\n",
    "        ns = f.read().strip()\n",
    "except Exception:\n",
    "    pass\n",
    "print('Using namespace:', ns)\n",
    "\n",
    "# 1) Secret with S3 creds + endpoint URL used by KServe storage initializer\n",
    "secret_name = 's3-credentials'\n",
    "secret_body = client.V1Secret(\n",
    "    metadata=client.V1ObjectMeta(name=secret_name),\n",
    "    type='Opaque',\n",
    "    string_data={\n",
    "        'AWS_ACCESS_KEY_ID': cfg.minio_access_key,\n",
    "        'AWS_SECRET_ACCESS_KEY': cfg.minio_secret_key,\n",
    "        'AWS_ENDPOINT_URL': ('https://' if cfg.minio_secure else 'http://') + cfg.minio_endpoint\n",
    "    }\n",
    ")\n",
    "try:\n",
    "    core.create_namespaced_secret(ns, secret_body)\n",
    "    print('Created Secret', secret_name)\n",
    "except client.exceptions.ApiException as e:\n",
    "    if e.status == 409:\n",
    "        core.patch_namespaced_secret(secret_name, ns, secret_body)\n",
    "        print('Patched existing Secret', secret_name)\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "# 2) ServiceAccount that mounts the secret\n",
    "sa_name = 'minio-s3-sa'\n",
    "sa_body = client.V1ServiceAccount(\n",
    "    metadata=client.V1ObjectMeta(name=sa_name),\n",
    "    secrets=[client.V1ObjectReference(name=secret_name)]\n",
    ")\n",
    "try:\n",
    "    core.create_namespaced_service_account(ns, sa_body)\n",
    "    print('Created ServiceAccount', sa_name)\n",
    "except client.exceptions.ApiException as e:\n",
    "    if e.status == 409:\n",
    "        core.patch_namespaced_service_account(sa_name, ns, sa_body)\n",
    "        print('Patched existing ServiceAccount', sa_name)\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "# 3) InferenceService for OVMS\n",
    "model_root = f\"s3://{cfg.minio_bucket}/{openvino_root_key}\"\n",
    "isvc_name = f\"{cfg.model_name}-ovms\"\n",
    "isvc_spec = {\n",
    "  'apiVersion': 'serving.kserve.io/v1beta1',\n",
    "  'kind': 'InferenceService',\n",
    "  'metadata': {'name': isvc_name},\n",
    "  'spec': {\n",
    "    'predictor': {\n",
    "      'model': {\n",
    "        'runtime': 'ovms',\n",
    "        'protocolVersion': 'v2',\n",
    "        'storageUri': model_root\n",
    "      },\n",
    "      'serviceAccountName': sa_name\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "group = 'serving.kserve.io'\n",
    "version = 'v1beta1'\n",
    "plural = 'inferenceservices'\n",
    "\n",
    "try:\n",
    "    custom.create_namespaced_custom_object(group, version, ns, plural, isvc_spec)\n",
    "    print('Created InferenceService', isvc_name)\n",
    "except client.exceptions.ApiException as e:\n",
    "    if e.status == 409:\n",
    "        custom.patch_namespaced_custom_object(group, version, ns, plural, isvc_name, isvc_spec)\n",
    "        print('Patched existing InferenceService', isvc_name)\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "# Wait for Ready and fetch URL\n",
    "def get_isvc_status():\n",
    "    obj = custom.get_namespaced_custom_object(group, version, ns, plural, isvc_name)\n",
    "    return obj.get('status', {})\n",
    "\n",
    "print('Waiting for InferenceService to be Ready...')\n",
    "url = None\n",
    "for _ in range(60):  # ~5-10 minutes max depending on cluster\n",
    "    st = get_isvc_status()\n",
    "    conditions = st.get('conditions', [])\n",
    "    if any(c.get('type') == 'Ready' and c.get('status') == 'True' for c in conditions):\n",
    "        url = st.get('url')\n",
    "        break\n",
    "    time.sleep(10)\n",
    "\n",
    "print('InferenceService URL:', url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test inference (V2 protocol)\n",
    "This probes the model metadata to discover input names and sends a dummy request with zeros.\n",
    "\n",
    "> Adjust `payload` to your real feature vector and dtypes for production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json, numpy as np\n",
    "\n",
    "assert url, 'InferenceService URL not found. Check the events/logs of the service.'\n",
    "model_endpoint = url.rstrip('/')\n",
    "name = cfg.model_name + '-ovms'\n",
    "\n",
    "# Try to get metadata\n",
    "meta = requests.get(f\"{model_endpoint}/v2/models/{name}\", timeout=30)\n",
    "if meta.status_code != 200:\n",
    "    # Some runtimes expose metadata at /versions/1 or use model name without suffix\n",
    "    alt_name = cfg.model_name\n",
    "    meta = requests.get(f\"{model_endpoint}/v2/models/{alt_name}\", timeout=30)\n",
    "    name = alt_name if meta.status_code == 200 else name\n",
    "\n",
    "print('Metadata status:', meta.status_code)\n",
    "if meta.ok:\n",
    "    print('Metadata:', meta.json())\n",
    "\n",
    "# Determine input signature\n",
    "input_name = 'input'\n",
    "datatype = 'FP32'\n",
    "shape = [1, n_features]\n",
    "try:\n",
    "    j = meta.json()\n",
    "    if 'inputs' in j and j['inputs']:\n",
    "        input_name = j['inputs'][0].get('name', input_name)\n",
    "        datatype = j['inputs'][0].get('datatype', datatype)\n",
    "        shape = j['inputs'][0].get('shape', shape)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "dummy = [0.0] * (shape[-1] if isinstance(shape, list) and len(shape) > 0 else n_features)\n",
    "payload = {\n",
    "  'inputs': [{\n",
    "      'name': input_name,\n",
    "      'shape': shape,\n",
    "      'datatype': datatype,\n",
    "      'data': dummy\n",
    "  }]\n",
    "}\n",
    "\n",
    "infer_url = f\"{model_endpoint}/v2/models/{name}/infer\"\n",
    "resp = requests.post(infer_url, json=payload, timeout=60)\n",
    "print('Infer status:', resp.status_code)\n",
    "print('Response:', resp.text[:1000])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
